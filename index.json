[{"authors":["admin"],"categories":null,"content":"I am a final year undergraduate student of computer science at BITS Pilani, Hyderabad Campus. I am currently the class valedictorian and have also served as the captain of the tennis team and as the vice chairperson of the IEEE student branch.\nI am currently pursuing my undergraduate thesis at the Max Planck Institute for Informatics under Dr. Andrew Yates and Dr. Paramita Mirza.\nI was recently a part of the IUSSTF-Viterbi programme 2019!\nMy research interests include reinforcement learning (RL), inverse RL and meta RL. I like thinking about and working on different applications of RL in the real world \u0026ndash; like dialogue systems, robotics and personalized learning. A lot of my work also involves NLP and IR and I am very interested in exploring the intersection of NLP/IR and RL. In the future, I hope to conduct some theoretical research in RL as well.\nIn the past I\u0026rsquo;ve had the good fortune of working closely with some amazing researchers such as Prof. N. L. Bhanu Murthy, Prof. Aruna Malapati, Prof. Kallirroi Georgila and Prof. David Traum.\nIf you are interested in my work or would like to chat about technical interests we might share, feel free to get in touch!\n","date":1572817224,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1572817224,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://skandavaidyanath.github.io/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"I am a final year undergraduate student of computer science at BITS Pilani, Hyderabad Campus. I am currently the class valedictorian and have also served as the captain of the tennis team and as the vice chairperson of the IEEE student branch.\nI am currently pursuing my undergraduate thesis at the Max Planck Institute for Informatics under Dr. Andrew Yates and Dr. Paramita Mirza.\nI was recently a part of the IUSSTF-Viterbi programme 2019!","tags":null,"title":"Skanda Vaidyanath","type":"authors"},{"authors":null,"categories":null,"content":"Hi! Thank you for taking the time out to check out my RL course. I hope to cover several RL fundamentals and algorithms as a part of this course in a little bit more theoretical detail than I normally come across in other similar tutorials. Having said that, I will also include some code samples and implementational details wherever I see fit.\nWe\u0026rsquo;ll start off with a couple of introduction posts and then cover several common RL algorithms and deep RL algorithms as well. I will try and put in code samples (Python3 and PyTorch) and mathematical proofs whenever possible.\nI will add references at the end of each lecture but most of the content for this course comes from Reinforcement Learning: An Introduction by Sutton and Barto and this NPTEL course by Prof. Balaraman Ravindran.\nAs I am currently conducting research in the field of RL, I will be giving some insights into how people use RL in their work and how they tackle RL problems. I will give detials about modeling the state space, action space and reward functions for an RL problem and other practical challenges one may face as well.\nI highly recommend some background in probability and maybe even some machine learning (supervised and unsupervised learning) and optimization before you get started with this course (not a lot, but some basics would be nice). A basic understanding of neural networks and backpropagation would be useful for the deep RL posts. None of this is absolutely essential to get an intuitive idea of RL but would be useful to have a thorough understanding.\nPlease let me know if there\u0026rsquo;s anything different you\u0026rsquo;d like to see or if I\u0026rsquo;ve made any mistakes.\n","date":1536451200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1536451200,"objectID":"5ef3f499275ec8479ed5a06eb3645c92","permalink":"https://skandavaidyanath.github.io/courses/rl-course/","publishdate":"2018-09-09T00:00:00Z","relpermalink":"/courses/rl-course/","section":"courses","summary":"A reinforcement learning course that will teach you everything you need right from the very basics till the most complicated algorithms that are in use today.","tags":null,"title":"Course Overview","type":"docs"},{"authors":null,"categories":null,"content":" Hi and welcome to the first post of this RL course. In this post, my aim will be to introduce the idea of RL to you and talk about the problems it solves and why its important.\nReinforcement learning is like that little-known cousin of supervised learning and unsupervised learning. Or at least it was for the longest time. In recent times though, its been gaining a lot of attention, mainly due to DeepMind\u0026rsquo;s AlphaZero.\nBut having said that, people still don\u0026rsquo;t quite know what RL is yet and don\u0026rsquo;t know how and when to use it. So as a part of this introductory blog, I will try to answer three questions that people often ask me about RL\n What is Reinforcement Learning? How is it different from Supervised learning or Unsupervised learning? What problems can it solve?  And lets begin!\nWhat is Reinforcement Learning? Reinforcement learning is a sub-part of Machine Learning (ML). The most general way to divide ML into three parts would be as Supervised learning (SL), Unsupervised learning (USL) and Reinforcement Learning. But most people only talk about SL and USL when they talk about ML. So my first job is to explain why the third paradigm is important and how it is different from the first two.\n Reinforcement learning is an area of machine learning concerned with how software agents ought to take actions in an environment in order to maximize some notion of cumulative reward.\n The above definition is taken from Wikipedia. The definition speaks about \u0026ldquo;agents\u0026rdquo; taking \u0026ldquo;actions\u0026rdquo; in \u0026ldquo;environments\u0026rdquo; to maximize \u0026ldquo;rewards\u0026rdquo;. But what does all this mean? Lets break it down, but before that, here is a simpler definition.\n Reinforcement learning is simply learning by trial and error.\n    The RL Setup. Source: Google Images   Think about how you started learning to ride a bike. You probably tried a bunch of different things and continued doing more of what worked (\u0026ldquo;worked\u0026rdquo; in this context probably means \u0026ldquo;did not fall and moved in the intended direction\u0026rdquo;) and less of what didn\u0026rsquo;t. Nobody gave you clear instructions on what to do at each step, you just tried things and they worked. In fact, humans gain several skills in the same fashion. Imagine you\u0026rsquo;re playing a brand new video game without reading the instructions or picking up a new sport. Humans learn several tasks by trial and error and that\u0026rsquo;s exactly what we\u0026rsquo;re trying to emulate with RL. Trying to get as close as possible to the way humans learn.\nWith that intuition, lets take a jab at the Wikipedia definition again. The \u0026ldquo;agent\u0026rdquo; in our biker example is the person trying to learn to ride a bike. The \u0026ldquo;environment\u0026rdquo; is everything that may affect the person riding the bike \u0026ndash; so this could be the road, the traffic, the weather, etc. As for \u0026ldquo;actions\u0026rdquo;, these are the different decisions the agent can make \u0026ndash; for example, they could be \u0026ldquo;turn left\u0026rdquo;, \u0026ldquo;turn right\u0026rdquo;, etc. The agent must decide based on the state of the environment, what the right action to play is at a given point. And finally \u0026ldquo;rewards\u0026rdquo; is some sort of feedback we get for the series of actions we just took. So we would get a positive reward if we reached our destination and negative if we fell down for example. All these terms will be dealt with more formally in the next post. For now, just make sure you get the intuition.\nBottomline: RL is just learning by trial and error to pick the right actions depending on the state of the environment.\nHow is it different from Supervised learning or Unsupervised learning? This is the question I get asked the most about RL. The difference between RL and USL is quite clear. In USL, there is absolutely no form of feedback or supervision whereas in RL we do get some sort of feedback in the form of a reward signal.\nSo Reinforcement Learning is not Unsupervised learning.\nThe more pertinent question is \u0026ndash; how is it different from supervised learning? Is RL just SL with class labels given in a different manner?\nLets look at another very common example. We want to teach our computer to play Tic-Tac-Toe. Our data is in the form of several games that have been played from start to finish. If we consider this as training data for our SL model, the only labels we could possibly decipher from these games would be the final outcome \u0026ndash; the winner of the game or if it was a draw. If we are able to somehow encode the game and train a classifier on the data, this SL model would be able to predict the outcome of a game (which is not very useful) but not how to play the game.\nIf we wanted to train a SL model to learn how to play the game, we would need training data in the form of the best move to play at every board position. But alas, we do not have such information and this is the case in most problems (think about riding a cycle or playing chess or a video game).\nSo what do we do now? For now, take it for granted that RL can solve the problem with the same data and teach a computer how to play Tic-Tac-Toe. We\u0026rsquo;ll talk about how it does it soon enough.\nBut since our aim was just to show that RL and SL are not the same, we are done here. SL requires \u0026ldquo;step-wise\u0026rdquo; (this is not a technical term and hence is in quotes, but you get the idea) labels to learn how to do a task. It requires \u0026ldquo;strong\u0026rdquo; supervision. RL can do the same thing with some sort of \u0026ldquo;weak\u0026rdquo;/\u0026ldquo;distant\u0026rdquo;/\u0026ldquo;semi\u0026rdquo;-supervision.\nSo Reinforcement Learning is not Supervised learning either.\nWhat problems can it solve? All this sounds great but what problems can RL solve? So far we\u0026rsquo;ve spoken about riding bikes and playing chess and video games but are there any significant real-world problems RL can solve?\nAs it turns out there are several. RL is also commonly referred to as Sequential Decision Making or Decision Making under Uncertainty. When we put it this way, we can think of several applications for RL in the real-world. I\u0026rsquo;ll talk about a few here.\n    Atari Games: Pong. Source: Google Images      AlphaZero: Chess. Source: Google Images   RL gained massive popularity because of its success at playing Go and Chess and Atari Games as well but there are several other applications of RL.\nRL is extremely versatile and can be used along with several other common ML areas like Computer Vision (CV) and Natural Language Processing (NLP).\nThe best example of using RL with CV is probably self-driving cars. With NLP, it can be used in dialogue systems (I recommend you Google how both of this is done to get a better idea). Another massive application area is in robotics and control. It can be used to train multi-agent systems, for example, a swarm of drones communicating with each other. One of my favourite applications of RL is personalized learning where an RL agent can design an optimal course for a student with the right number of tests/assignments administered at the right time to encourage maximum learning. There is similar work being done on personalized healthcare as well.\n   Robotics. Source: Google Images   And so there are a ton of different applications you can make using RL. There are also other slightly different uses, for example, RL can be used to overcome non-differentiable steps in deep learning.\nAnd with that, we\u0026rsquo;ve answered all the questions that we set out to! We get into a lot more details in the next post so make sure you take a look at that as well.\nFeel free to let me know if you have any feedback!\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"e337f5ca8a8c27baee01a7b4383b32fe","permalink":"https://skandavaidyanath.github.io/courses/rl-course/post1/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/rl-course/post1/","section":"courses","summary":"Hi and welcome to the first post of this RL course. In this post, my aim will be to introduce the idea of RL to you and talk about the problems it solves and why its important.\nReinforcement learning is like that little-known cousin of supervised learning and unsupervised learning. Or at least it was for the longest time. In recent times though, its been gaining a lot of attention, mainly due to DeepMind\u0026rsquo;s AlphaZero.","tags":null,"title":"Introduction: Why RL?","type":"docs"},{"authors":null,"categories":null,"content":" In this post, we\u0026rsquo;ll try to get into the real nitty-gritties of RL and build on the intuition that we gained from the last article. So we\u0026rsquo;ll bring in some mathematical foundation and then introduce some RL parlance that we will use for the rest of this course. I strongly recommend that you keep referring back to this post in case you need a refresher on RL terms and symbols. I\u0026rsquo;ll stick to the standard notation from the RL book.\nBefore we begin with the mathematical foundations of RL, I\u0026rsquo;d like to point out some issues with RL and what kinds of problems we need to account for if we were to come up with RL algorithms of our own. Once again, I\u0026rsquo;m going to move on to a new example so lets take Chess this time. So we want to teach our agent (recall what this means from the previous post) how to play the game of chess. Lets assume we have some sort of reward function in place where we get some small positive rewards for capturing a piece and small negative rewards for losing a piece depending on the importance of the piece (so losing a queen would lead to a negative reward of larger magnitude than losing a pawn). We also have some large positive final reward for winning the game and a large positive negative reward for losing. If you\u0026rsquo;re wondering whether just ths large final reward is a sufficient reward function on its own, you\u0026rsquo;re probably right and it probably is, but lets stick to this for the sake of illustration. Now assume we have an RL algorithm that can look at several games of Chess and the rewards and learn to play Chess on its own. What would this algorithm need to account for? We spoke about trial and error being the basis of any RL algorithm in the previous post that is exactly what our algorithm would do as well. It starts playing random moves and when it plays a good move (positive reward), it remembers to play that move the next time it is in a similar situation. This seems fine on the face of it, but there is an issue. Maybe the algorithm found a good move to play at a given position, but what if there was a better move? We need some way for the algorithm to account for the possibility of there being a better move than the one it has found already. So when we train our agent we need to make sure the agent doesn\u0026rsquo;t greedily play the best move it knows all the time but also plays some different moves, hoping that they may be better than the one it already found. This is called the exploration-exploitation tradeoff in RL. Usually, RL algorithms tend to explore i.e. play many random moves initially and when the agent is more sure about the best moves under different circumstances, it starts exploiting that knowledge.\nLets move on to the next issue that our RL algorithm will have to account for. Lets say our RL algorithm is learning from a game of Chess again where the player sacrifices the queen but goes on to win the game. The RL agent immediately registers a negative reward for the loss of the queen but the large positive reward for winning the game only comes much later. But it is entirely possible that the very queen sacrifice that the RL agent probably classified as a bad move, was the reason for the player winning the game. How do we account for this in our algorithm? This is the concept of delayed rewards and we will deal with a simple yet elegant solution for this as well as we go through this post.\nWith that background, lets talk about how RL problems are modeled and get into some math.\nMarkov Decision Processes     A Markov Decision Process. Source: here      The RL Framework (with some additional details). Source: Google Images   Almost all RL problems can be modeled as a Markov Decision Process (MDP). So what is an MDP? An MDP can be defined as a five tuple $$\\langle \\mathcal{S}, \\mathcal{A}, \\mathcal{P}, \\mathcal{R}, \\gamma \\rangle$$ Lets take a closer look at what all of this means.\n S : This is the set of states of the MDP. In an RL setting, this would correspond to different settings of the environment. In the previous post, we spoke about how RL was all about choosing the right actions at the right times i.e. depending on the state of the environment. This is the state we were talking about. A state in chess or tic-tac-toe cpuld be the board positions or while riding a bike could be some combination of the pertinent variables like the angle of the bike with thr ground, the wind speed, etc. The S variable represents the set of all unique states in the MDP. A : This is the set of all actions of the MDP. We already spoke about actions briefly. Actions describe the possible moves in a game of Chess or tic-tac-toe or different arrow keys or buttons in a video game, etc. It represents the different options the agent has and can play at a given point in time. A represents the set of all unique actions available to the agent.  Before we move on to the other symbols, lets get some things clear. Here is another more compilacted MDP for your reference.\n   Another MDP. Source: Google Images   Some of the states of the MDP are designated as start states or initial states and end states or terminal states. An episode in RL is a sequence of state-action pairs that take the agent from a start state to a terminal state. So the agent starts from one of the intial states, plays an action, goes to the next state and so on until it hits a terminal state and the episode ends. Now lets take a look at this MDP in the diagram above. Assume S0 is your initial state. Notice that a0 from S0 has two arrows, one going into S0 again and another going into S2. The numbers on the arrows indicate 0.5 and 0.5 respectively. This means that if an agent plays the action a0 from S0, it has a 0.5 probability that it ends up back in S0 and a 0.5 probability that it ends up in S2. And similarly we have arrows going all over the diagram. Also notice the wiggly arrows \u0026ndash; they\u0026rsquo;re rewards. Notationally, we index the sequence of state-action pairs in an episode with a time variable t so we say an agent plays action at from state st abnd gets reward rt+1 for doing so (the reward can be 0). Here, t starts from 0 and we represent the terminal time-step as T.\n P : Now P is the probability function defined as P(s\u0026lsquo;| s, a) which is read as the probability of moving to \u0026ldquo;state s\u0026rsquo;\u0026rdquo; from \u0026ldquo;state s\u0026rdquo; if the agent plays \u0026ldquo;action a\u0026rdquo;. So for example, P(S2|S0,a0) = 0.5 R : This is the reward function and is defined as R(s\u0026lsquo;| s, a) which is the reward the agent gets for moving to \u0026ldquo;state s\u0026rsquo;\u0026rdquo; from \u0026ldquo;state s\u0026rdquo; if the agent plays \u0026ldquo;action a\u0026rdquo;. So for example, R(S0|S1,a0) = +5 $\\gamma$ : We spoke about the concept of delayed rewards earlier in the post and we wanted a way to accound for delayed effects of actions. This is where $\\gamma$ helps. We define the returns of an action from a given state as the sum of the discounted rewards we receive from that state for playing that action. If we started from the state s0, the returns would be defined as r1 + $\\gamma$ r2 + $\\gamma$2 r3 + \u0026hellip; $\\gamma$ T-1 rT. We use the word \u0026ldquo;discounted\u0026rdquo; because $\\gamma$ is usually a number between 0 and 1 and with the increasing powers, we give more weight to the immediate rewards than the delayed rewards. Hence, $\\gamma$ is also called the discounting factor. The symbol we use for returns from timestep t is usually Gt although some people like using R as well (r for reward and R for returns). We will stick to the former notation. Now going back to the queen sacrifice example, if we were to consider the returns in our algorithm instead of just the immediate reward, we will be able to account for the delayed positive effect and not just the immediate negative effect.  And with that, we\u0026rsquo;ve covered MDPs and how to model RL problems. But with that definition, we still haven\u0026rsquo;t accounted for the exploration-exploitation tradeoff. So in the next post, we\u0026rsquo;ll introduce a few more symbols and definitions and get cracking with our very first RL algorithm!\nOnce again, let me know if you have any feedback or suggestions.\nReferences  A (Long) Peek into Reinforcement Learning by Lilian Weng  ","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"33f160f8a2391efe7fee696290f16328","permalink":"https://skandavaidyanath.github.io/courses/rl-course/post2/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/rl-course/post2/","section":"courses","summary":"In this post, we\u0026rsquo;ll try to get into the real nitty-gritties of RL and build on the intuition that we gained from the last article. So we\u0026rsquo;ll bring in some mathematical foundation and then introduce some RL parlance that we will use for the rest of this course. I strongly recommend that you keep referring back to this post in case you need a refresher on RL terms and symbols.","tags":null,"title":"RL Fundamentals","type":"docs"},{"authors":null,"categories":null,"content":"","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"927a996da4d04b81c6d6135676f0aab9","permalink":"https://skandavaidyanath.github.io/courses/rl-course/post3/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/rl-course/post3/","section":"courses","summary":"","tags":null,"title":"The Bellman Equation and Dynamic Programming","type":"docs"},{"authors":["Skanda Vaidyanath"],"categories":["Reinforcement Learning","Deep Learning","Information Retrieval"],"content":"Coming soon\u0026hellip;\n","date":1572817224,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1572817224,"objectID":"a7a8d69514281a00a73cf384646f75c1","permalink":"https://skandavaidyanath.github.io/post/works-like-a-charm/","publishdate":"2019-11-04T03:10:24+05:30","relpermalink":"/post/works-like-a-charm/","section":"post","summary":"Coming soon\u0026hellip;","tags":["Reinforcement Learning","Deep Learning","Information Retrieval"],"title":"Works Like a Charm","type":"post"},{"authors":["Skanda Vaidyanath"],"categories":["Reinforcement Learning","Deep Learning"],"content":" In this post, I will be talking about a unique way to use reinforcement learning (RL) in deep learning applications. I definitely recommend burshing up some deep learning fundamentals and if possible, some policy gradient fundamentals as well before you get started with this post.\nTraditionally, RL is used to solve sequential decision making problems or decision making problems under uncertainty. There are several applications in the fields of video games and robotics where the task at hand can be very easily seen as an RL problem and can be modeled appropriately as well. But RL as a technique is quite versatile and can be used in several other domains to train neural networks that are traditionally trained in a supervised fashion. We\u0026rsquo;ll talk about one very important such application in this post. Along the way, I\u0026rsquo;ll also try to convince you that this isn\u0026rsquo;t really a different way to use RL but rather just a different way to look at the traditional RL problem.\nNon-differentiable steps in deep learning: The Gaps Sometimes when we\u0026rsquo;re coming up with neural network architectures, we stumble up on non-differentiable operations as a part of our network. Now this is a problem as we can\u0026rsquo;t backpropagate losses through such operation and hence lets call these \u0026ldquo;gaps\u0026rdquo;. So what are some common gaps we come across in neural networks?\nBefore we start talking about some \u0026ldquo;real gaps\u0026rdquo;, its worth mentioning that the famous ReLU function is a non-differentiable function but we overcome that gap by setting the derivative at 0 to either 1 or 0 and get away with it.\nNow lets take a better example \u0026ndash; variational autoencoders (VAE). Without going into two many details, the VAE network outputs two vectors: a $\\mu$ vector and a $\\sigma$ vector and it involves a crucial sampling step where we sample from the distribution N($\\mu$, $\\sigma$) as a part of the network. Now sampling is a gap as it is a non-differentiable step. So how do they get over this in the VAE case? They use a clever trick. Instead of sampling from N($\\mu$, $\\sigma$), they just rewrite this as $\\mu$ + $\\sigma$N(0,1) where they sample from the standard normal function. This neat trick now makes the expression differentiable because we just need the $\\mu$ and $\\sigma$ quantities to be differentiable and we don\u0026rsquo;t care about the N(0,1). Remember that we only need to differentiate with respect to the parameters of our network (brush up some backpropagation basics if you\u0026rsquo;re confused here) and hence we need to differentiate wrt $\\mu$ and $\\sigma$. For more details about VAEs read this post or this one.\nSo as it turned out, that wasn\u0026rsquo;t a very good example either but we\u0026rsquo;re starting to understand what we mean by gaps now and how common they are. Some common examples of gaps in networks are sampling operations and the argmax operation. Read this thread to understand why max is a differentiable function (recall max pooling in CNNs) while argmax is not. These are commonly used in natural language processing (NLP) applications, information retrieval (IR) applications and Computer Vision (CV) applications as well. For example, a sampling function could be used to select words from a sentence based on a probability distribution in an NLP application or an argmax function could be used to find the highest ranked document in an IR application. Hard attention uses sampling techniques which involves non-differentiable computation.\nSo its quite clear that these gaps are common in several deep learning architectures and sometimes, it could even be useful to introduce such a gap in the network intentionally to reap added benefits. The only question is, how do we bridge these gaps?\nReinforcement Learning and Policy Gradients: The Bridge Policy gradients are a class of algorithms in RL. There are several policy gradient algorithms and this is a great blog that lists out almost all of them. But without going into too many details, these algorithms work in the policy space by updating the parameters of the policy we\u0026rsquo;re trying to learn. That means we don\u0026rsquo;t necessarily need to find the value function of different states but we can directly alter our policy until we\u0026rsquo;re happy. The most common policy gradient (PG) algorithm is the REINFORCE which is a Monte Carlo algorithm. This means we run an entire episode and make changes to our policy only at the end of each episode and not at every step. We make these changes based on the returns that we received by taking a given action from a given state in the episode. I skip the derivation of the policy gradient here but it can be found in the link above. The final result is in the image below.\n   The Policy Gradient. Source: here   The key idea here is that in policy gradient methods, we are allowed to sample different actions from a given state and wait till the end of an episode before we make updates to our network. So if we have a sampling operation as a part of our network, we can introduce a policy gradient and think of it as sampling actions in a given state in an RL framework. A similar procedure can also be followed if we had argmax in place of the sampling operation.\nConsider a neural network now with a gap. The images below are taken from this blog on Policy Gradients written by Andrej Karpathy.\n    Gaps in a neural network. Source: Karpathy\u0026rsquo;s blog      The sampling operation. Source: Karpathy\u0026rsquo;s blog   So now we can train the blue arrows i.e. the differentiable path as usual. But to train the red arrows, we need to introduce a policy gradient and as we sample, we ensure with the help of the policy gradient that we encourage samples that led to a lower loss. The updates to the red arrows happen independently than those of the blue arrows. Note that in the diagrams above, there isn\u0026rsquo;t really a gap per-say because the blue arrows go all the way from start to finish. So there is a differentiable path and a non-differentiable path. A true gap would mean there would be no completely differentiable path at all. In this case, we need to make sure that the loss functions on either side of the gap are \u0026ldquo;in sync\u0026rdquo; and are being optimized in such a way that it facilitates joint training and achieves a common goal.\nWe can still look at this entire setup as a traditional RL problem if we think of the inputs to the neural network as the state and the sampling process as sampling different actions from that given state. Now what is the reward function? This depends on what comes after the gap and could be an output from the rest of the network or it could be a completely independent reward function that you came up with as well. So at the end of the day, it is still the same MDP with the traditional setup but just used in a very different way.\n","date":1572817209,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1572817209,"objectID":"ac71a2b9e1bf254e738c35f279e48051","permalink":"https://skandavaidyanath.github.io/post/bridging-the-gaps-with-rl/","publishdate":"2019-11-04T03:10:09+05:30","relpermalink":"/post/bridging-the-gaps-with-rl/","section":"post","summary":"In this post, I will be talking about a unique way to use reinforcement learning (RL) in deep learning applications. I definitely recommend burshing up some deep learning fundamentals and if possible, some policy gradient fundamentals as well before you get started with this post.\nTraditionally, RL is used to solve sequential decision making problems or decision making problems under uncertainty. There are several applications in the fields of video games and robotics where the task at hand can be very easily seen as an RL problem and can be modeled appropriately as well.","tags":["Reinforcement Learning","Deep Learning"],"title":"Bridging the Gaps With Reinforcement Learning","type":"post"},{"authors":[],"categories":[],"content":"This project is for my undergraduate thesis at the Max Planck Institute for Informatics under Dr. Andrew Yates and Dr. Paramita Mirza and is currently in progress.\n","date":1572563798,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1572563798,"objectID":"68fac9c2ace1f0628f0573b9719337a6","permalink":"https://skandavaidyanath.github.io/project/mpii/","publishdate":"2019-11-01T04:46:38+05:30","relpermalink":"/project/mpii/","section":"project","summary":"This project is for my undergraduate thesis at the Max Planck Institute for Informatics under Dr. Andrew Yates and Dr. Paramita Mirza and is currently in progress.","tags":[],"title":"Undergraduate Thesis - MPII","type":"project"},{"authors":["Skanda Vaidyanath"],"categories":["Natural Language Processing","Machine Learning","Software Engineering"],"content":"This project is currently in progress and a research paper is in preparation. I will attach the code, the dataset and the paper once it has been published. This project was done under the guidance of Prof. N. L. Bhanu Murthy.\n","date":1569888000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1569888000,"objectID":"6feeb61f6ca177ef62c178ab97287464","permalink":"https://skandavaidyanath.github.io/project/bug-prediction/","publishdate":"2019-10-01T00:00:00Z","relpermalink":"/project/bug-prediction/","section":"project","summary":"Extracting features from source code for bug detection in software projects","tags":["Natural Language Processing","Other"],"title":"Software Bug Prediction","type":"project"},{"authors":["Skanda Vaidyanath"],"categories":null,"content":"","date":1563667200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1563667200,"objectID":"f7f480e433b065fc715fb317ce293703","permalink":"https://skandavaidyanath.github.io/talk/usc-ict/","publishdate":"2020-01-02T18:57:28+05:30","relpermalink":"/talk/usc-ict/","section":"talk","summary":"Developed RL policies to control a swarm of drones to save humans from a forest fire. For the [IUSSTF-Viterbi programme](https://www.iusstf.org/program/iusstf-viterbi-program) 2019","tags":["Reinforcement Learning"],"title":"The Human-Swarm Project","type":"talk"},{"authors":["Skanda Vaidyanath"],"categories":["Reinforcement Learning"],"content":"This project is currently in progress and a research paper is in preparation. I will attach the code, the slides and the paper once it has been published. This project was done under the guidance of Prof. Kallirroi Georgila and Prof. David Traum.\n","date":1563580800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1563580800,"objectID":"74071acae3f5fc56c084ff6d91b9a392","permalink":"https://skandavaidyanath.github.io/project/usc-ict/","publishdate":"2019-07-20T00:00:00Z","relpermalink":"/project/usc-ict/","section":"project","summary":"Developed RL policies to control a swarm of drones to save humans from a forest fire. For the [IUSSTF-Viterbi programme](https://www.iusstf.org/program/iusstf-viterbi-program) 2019","tags":["Reinforcement Learning"],"title":"The Human Swarm Project","type":"project"},{"authors":["Skanda Vaidyanath"],"categories":["Machine Learning"],"content":"This blog post has some interesting details. The code and a PDF report are available in links above. This project was done by Vamsi Aribandi and myself.\n","date":1561939200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561939200,"objectID":"2608d231e9e4167041a46508aa029129","permalink":"https://skandavaidyanath.github.io/project/brain-decoding/","publishdate":"2019-07-01T00:00:00Z","relpermalink":"/project/brain-decoding/","section":"project","summary":"Using machine learning techniques to classify brain signals of patients looking at either a blurry face or a clear face.","tags":["Other"],"title":"Brain Decoding","type":"project"},{"authors":["Skanda Vaidyanath"],"categories":["Natural Language Processing","Information Retrieval","Deep Learning"],"content":"This project is currently in progress and a research paper is in preparation. I will attach the code, the dataset and the paper once it has been published. This project was done under the guidance of Prof. N. L. Bhanu Murthy and Prof. Aruna Malapati.\n","date":1541030400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1541030400,"objectID":"1adb91b16938dfc03aa1a2122b624eb4","permalink":"https://skandavaidyanath.github.io/project/cqa/","publishdate":"2018-11-01T00:00:00Z","relpermalink":"/project/cqa/","section":"project","summary":"Developed siamese deep learning architectures to find similar questions from a Q\u0026A archive on a distance learning platform","tags":["Natural Language Processing","Information Retrieval","Deep Learning"],"title":"Community Question Answering for a distance-learning platform","type":"project"},{"authors":["Skanda Vaidyanath"],"categories":["Natural Language Processing","Deep Learning"],"content":"Python notebook and slides for the project are linked above. This project was done as a part of the applications for the MITACS Globalink programme.\n","date":1539993600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1539993600,"objectID":"59bded0ed6c4053f35ff438725d8c52f","permalink":"https://skandavaidyanath.github.io/project/mitacs/","publishdate":"2018-10-20T00:00:00Z","relpermalink":"/project/mitacs/","section":"project","summary":"Designing a curriculum for job applicants by analysing data on job descriptions. Assignment done for the [MITACS Globalink programme](https://www.mitacs.ca/en/programs/globalink)","tags":["Natural Language Processing","Deep Learning"],"title":"Personalized Learning from Job Descriptions","type":"project"},{"authors":["Skanda Vaidyanath"],"categories":["Natural Language Processing","Deep Learning","Information Retrieval"],"content":"The code for this project cannot be made public. However, you can find a technical report and some slides in the links above. This project was done at the Indira Gandhi Centre for Atomic Research (IGCAR).\n","date":1532995200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1532995200,"objectID":"6be0d21f3e3e49b42b95607ba42785b2","permalink":"https://skandavaidyanath.github.io/project/igcar/","publishdate":"2018-07-31T00:00:00Z","relpermalink":"/project/igcar/","section":"project","summary":"Search engine on a nuclear corpus and an outline of an approach to build a factoid-based question answering system","tags":["Natural Language Processing","Deep Learning","Information Retrieval"],"title":"Search Engine On a Nuclear Corpus","type":"project"},{"authors":["Skanda Vaidyanath"],"categories":null,"content":"","date":1532476800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1532476800,"objectID":"abfc868298a167f3d4e1c579b8771b60","permalink":"https://skandavaidyanath.github.io/talk/igcar/","publishdate":"2020-01-02T18:57:37+05:30","relpermalink":"/talk/igcar/","section":"talk","summary":"Search engine on a nuclear corpus and an outline of an approach to build a factoid-based question answering system","tags":["Natural Language Processing","Deep Learning","Information Retrieval"],"title":"Developing a Search Engine on a nuclear corpus and outlining a semantic based approach to Entity Profiling from raw text to build a Question Answering system","type":"talk"}]