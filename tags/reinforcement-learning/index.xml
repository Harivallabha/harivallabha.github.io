<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Reinforcement Learning | Skanda Vaidyanath</title>
    <link>https://skandavaidyanath.github.io/tags/reinforcement-learning/</link>
      <atom:link href="https://skandavaidyanath.github.io/tags/reinforcement-learning/index.xml" rel="self" type="application/rss+xml" />
    <description>Reinforcement Learning</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Tue, 21 Jan 2020 06:02:24 +0530</lastBuildDate>
    <image>
      <url>https://skandavaidyanath.github.io/img/icon-192.png</url>
      <title>Reinforcement Learning</title>
      <link>https://skandavaidyanath.github.io/tags/reinforcement-learning/</link>
    </image>
    
    <item>
      <title>An Overview of Bandits</title>
      <link>https://skandavaidyanath.github.io/post/bandits/</link>
      <pubDate>Tue, 21 Jan 2020 06:02:24 +0530</pubDate>
      <guid>https://skandavaidyanath.github.io/post/bandits/</guid>
      <description>&lt;p&gt;Coming soon&amp;hellip;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>The Inverse Problem</title>
      <link>https://skandavaidyanath.github.io/post/inverse-rl-paper/</link>
      <pubDate>Sun, 19 Jan 2020 00:17:50 +0530</pubDate>
      <guid>https://skandavaidyanath.github.io/post/inverse-rl-paper/</guid>
      <description>

&lt;p&gt;This is a review of the paper &lt;a href=&#34;https://ai.stanford.edu/~ang/papers/icml00-irl.pdf&#34; target=&#34;_blank&#34;&gt;Algorithms for Inverse Reinforcement Learning&lt;/a&gt;. I recommend some reinforcement learning (RL) basics before you read this. The first couple of posts from the RL course on my page might be a good starting point.&lt;/p&gt;

&lt;p&gt;Inverse RL (IRL) is a topic I&amp;rsquo;ve been interested in in recent times so I&amp;rsquo;m excited to write this post. So lets get cracking!&lt;/p&gt;

&lt;h2 id=&#34;the-problem&#34;&gt;The Problem&lt;/h2&gt;














&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;the_problem.png&#34; data-caption=&#34;The Inverse RL problem. Source: Google Images&#34;&gt;
&lt;img src=&#34;the_problem.png&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    The Inverse RL problem. Source: Google Images
  &lt;/figcaption&gt;


&lt;/figure&gt;


&lt;p&gt;Reinforcement Learning has allowed researchers to solve several challenging problems without direct supervision but with some sort of distant/ weak supervision or feedback signal. In RL, this feedback signal is in the form of a reward signal. Reward functions are crucial to developing powerful RL models but coming up with good reward functions can be a challenging task. If we were to taake a game like Tic Tac Toe, the reward signal just presents itself based on the result of the game. If it were a video game we were trying to learn, then once again the score in the game provides a solid reward signal. But what about in the case of self-driving cars? With so many factors to consider, it is difficult to come up with a good reward function and even if we do, there may be a better reward function possible. Researchers recognized this issue with RL and decided to come up with a way to &lt;em&gt;learn a reward function&lt;/em&gt;. They wanted to learn a reward function from optimal behaviour. So they would look at a human driving a car, learn a reward function from the demonstration, and then use this reward function to train an RL agent. The problem of extracting the reward fnction from observed optimal behaviour is the problem of Inverse Reinforcement Learning (IRL).&lt;/p&gt;














&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;problem_definition.PNG&#34; data-caption=&#34;The problem definition. Source: The paper&#34;&gt;
&lt;img src=&#34;problem_definition.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    The problem definition. Source: The paper
  &lt;/figcaption&gt;


&lt;/figure&gt;


&lt;blockquote&gt;
&lt;p&gt;Inverse Reinforcement Learning: Find a reward function to explain observed optimal behaviour&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;The paper gives two major motivations to learn such a reward function. One is obvious: to use the reward function to train RL agents. Two, is to use with apprenticeship learning or imitation learning to teach agents.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;The reward function and not the policy is the most succinct, robust and transferable definition of a task&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&#34;some-quick-pointers&#34;&gt;Some Quick Pointers&lt;/h2&gt;

&lt;p&gt;Before we get into the meat of the paper, here are some quick pointers to keep in mind.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;All reward functions are only functions of the state and not the state and the action. So we have &lt;em&gt;R(s)&lt;/em&gt; and not &lt;em&gt;R(s,a)&lt;/em&gt; everywhere. This is done to simplify the math and the extension is simple. It may help to think of &lt;em&gt;R&lt;/em&gt; as a vector of size &lt;em&gt;N&lt;/em&gt; where &lt;em&gt;N&lt;/em&gt; is the number of states.&lt;/li&gt;
&lt;li&gt;All values of the reward vector are bounded by a magnitude of &lt;em&gt;R&lt;/em&gt;&lt;sub&gt;max&lt;/sub&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;P&lt;/em&gt;&lt;sub&gt;a&lt;/sub&gt; is a &lt;em&gt;NxN&lt;/em&gt; matrix where &lt;em&gt;P&lt;/em&gt;&lt;sub&gt;&lt;em&gt;ij&lt;/em&gt;&lt;/sub&gt; is the probability of going from state &lt;em&gt;i&lt;/em&gt; to state &lt;em&gt;j&lt;/em&gt; by playing action &lt;em&gt;a&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;The paper uses the MDP setup for its proofs and arguments. Here are some properties and theorems that they take advantage of.&lt;/li&gt;
&lt;/ul&gt;














&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;mdp_theorems.PNG&#34; data-caption=&#34;MDP Theorems. Source: The paper&#34;&gt;
&lt;img src=&#34;mdp_theorems.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    MDP Theorems. Source: The paper
  &lt;/figcaption&gt;


&lt;/figure&gt;


&lt;h2 id=&#34;irl-in-finite-state-spaces&#34;&gt;IRL in finite state spaces&lt;/h2&gt;

&lt;p&gt;So we need to find a reward function that explains our optimal behaviour. In this case, let us assume we have a finite state space of size &lt;em&gt;N&lt;/em&gt;. The paper proves the following theorem using the properties above. The proof is quite simple to follow so I won&amp;rsquo;t talk about it here.&lt;/p&gt;














&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;soln_set.PNG&#34; data-caption=&#34;MDP Theorems. Source: The paper&#34;&gt;
&lt;img src=&#34;soln_set.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    MDP Theorems. Source: The paper
  &lt;/figcaption&gt;


&lt;/figure&gt;


&lt;p&gt;Now what does this theorem tell us? This theorem has now characterized our solution set. We&amp;rsquo;re no longer looking for a needle in a haystack. Initially, our reward vector &lt;em&gt;R&lt;/em&gt; could have been any real vector of size &lt;em&gt;N&lt;/em&gt;. But now, we have some constraints. &lt;em&gt;R&lt;/em&gt; has to satisfy the above condition.&lt;/p&gt;

&lt;p&gt;But the authors point out two issues.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;em&gt;R&lt;/em&gt;=0 is always a solution. If the reward function is the zero vector, then any policy is an optimal policy and so is our observed policy. The authors point out that this can be alleviated by demanding that our observed policy be the only optimal policy but this doesn&amp;rsquo;t work entirely because although we can get rid of the zero vector now, some vectors arbitrarily close to the zero vector would still be solutions.&lt;/li&gt;
&lt;li&gt;We have characterised our solution set, but there are still several vectors that satisfy this condition. Which of those is our reward function?&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;To address these issues the authors came up with a linear programming (LP) formulation to find the &amp;ldquo;best&amp;rdquo; &lt;em&gt;R&lt;/em&gt; vector. The authors sought to maximize the sum of the difference between the quality of the optimal action and the next best action.&lt;/p&gt;














&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;lp_term.PNG&#34; data-caption=&#34;The first term is the quality of the best action and the second term is the quality of the next best action. Source: The paper&#34;&gt;
&lt;img src=&#34;lp_term.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    The first term is the quality of the best action and the second term is the quality of the next best action. Source: The paper
  &lt;/figcaption&gt;


&lt;/figure&gt;


&lt;p&gt;So maximizing the above should give you the reward function but the authors also claim that smaller rewards lead to simpler reward functions and hence want to control the magnitude of the &lt;em&gt;R&lt;/em&gt; vector. To do this, they add a penalty term of $\lambda\Vert R\Vert_{1}$. $\lambda$ is a hyperparameter they control and larger the $\lambda$, the smaller the &lt;em&gt;R&lt;/em&gt; vector norm and simpler the reward function. But this is a trade-off since you also want to maximize the first term.
The authors claim that there is a phase transition point $\lambda_{o}$,
such that if $\lambda &amp;gt; \lambda_{o}$, &lt;em&gt;R&lt;/em&gt;=0 and if $\lambda &amp;lt; \lambda_{o}$, &lt;em&gt;R&lt;/em&gt; is bounded away from 0. So the best $\lambda$ would be a value just below $\lambda_{o}$.&lt;/p&gt;

&lt;p&gt;So the final optimization objective is as follows.&lt;/p&gt;














&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;opti.PNG&#34; data-caption=&#34;Final optimization problem for the finite state space case. Source: The paper&#34;&gt;
&lt;img src=&#34;opti.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Final optimization problem for the finite state space case. Source: The paper
  &lt;/figcaption&gt;


&lt;/figure&gt;


&lt;p&gt;Note that the summation term running from 1 to &lt;em&gt;N&lt;/em&gt; is so that we maximize across all the states in the MDP. This can be solved as a LP problem now.&lt;/p&gt;

&lt;h2 id=&#34;irl-in-large-state-spaces-using-linear-function-approximation&#34;&gt;IRL in large state spaces using linear function approximation&lt;/h2&gt;

&lt;p&gt;We now consider a large (possibly infinite) state space. Assume we have &lt;em&gt;n&lt;/em&gt; variables in our state space and so think of &lt;em&gt;R&lt;/em&gt; now as a function $\Re^n \rightarrow \Re$.&lt;/p&gt;

&lt;p&gt;But we don&amp;rsquo;t want to consider all such functions so let us restrict ourselves by only considering functions in the following format. We have used linear function approximation here.&lt;/p&gt;














&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;lfa.PNG&#34; data-caption=&#34;Linear function approximation for the reward function. Source: The paper&#34;&gt;
&lt;img src=&#34;lfa.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Linear function approximation for the reward function. Source: The paper
  &lt;/figcaption&gt;


&lt;/figure&gt;


&lt;p&gt;The \phi functions are basis functions over the state variables and our job now is to find the $\alpha$ values. Once again, since we&amp;rsquo;re using linear function approximation, we can use LP to solve the problem.&lt;/p&gt;

&lt;p&gt;The paper also shows that the value function under a given policy is also a linear function of the $\alpha$ values (refer to the paper to see why this is true). So now, we can rewrite the equation that characterizes our solution set as the following for the large state space case.&lt;/p&gt;














&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;gen_char_set.PNG&#34; data-caption=&#34;Characterizing the solution set for the large state space case. Source: The paper&#34;&gt;
&lt;img src=&#34;gen_char_set.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Characterizing the solution set for the large state space case. Source: The paper
  &lt;/figcaption&gt;


&lt;/figure&gt;


&lt;p&gt;But we have a problem now. This leads to infinitely many constraints to check because the state space could be infinite and we need to check the condition for each one of those states. Remember we had a summation in the final equation in the last section? That would be an infinite summation in this case. However, the authors circumvent this issue algorithmically by just sampling a large number of states and just checking for those states.&lt;/p&gt;

&lt;p&gt;The other issue is that since we have restricted ourselves by using linear function approximation, we may not be able to express all reward functions and hence we&amp;rsquo;ll relax some constraints and pay a penalty when we don&amp;rsquo;t meet the constraints. The final optimization objective is below.&lt;/p&gt;














&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;opti2.PNG&#34; data-caption=&#34;Final optimization problem for the large state space case. Source: The paper&#34;&gt;
&lt;img src=&#34;opti2.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Final optimization problem for the large state space case. Source: The paper
  &lt;/figcaption&gt;


&lt;/figure&gt;


&lt;p&gt;&lt;em&gt;S&lt;/em&gt;&lt;sub&gt;o&lt;/sub&gt; is the subsample of states and $p(x) = x$ when $x \geq 0$ and $2x$ when $x &amp;lt; 0$. This can be solved with LP now.&lt;/p&gt;

&lt;h2 id=&#34;irl-from-sampled-trajectories&#34;&gt;IRL from sampled trajectories&lt;/h2&gt;

&lt;p&gt;Now, we come to the most interesting and most realistic case. We now try to learn from sampled trajectories from the environment. &lt;strong&gt;We do not require an explicit model of the MDP but we do assume the ability to find an optimal policy under any reward function. We also assume the ability to simulate trajectories in the environment with the optimal policy or any other policy we want.&lt;/strong&gt; Also assume there is only a single start state &lt;em&gt;s&lt;/em&gt;&lt;sub&gt;o&lt;/sub&gt;. This is not a string assumption as if there are several start states with an initial state distribution, add an additional state and connect it to each of them.&lt;/p&gt;

&lt;p&gt;Once again, &lt;em&gt;R&lt;/em&gt; will be expressed as a linear function approximation in the same form as the previous section. Please refer to the paper and convince yourself that it is possible to use Monte Carlo trajectories to estimate a value function that is also linear in the $\alpha$ values. The math is quite simple and straight-forward. This is important because it allows us to use LP again.&lt;/p&gt;

&lt;p&gt;So now we have the optimization objective as follows.&lt;/p&gt;














&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;opti3.PNG&#34; data-caption=&#34;Final optimization problem for the trajectories case. Source: The paper&#34;&gt;
&lt;img src=&#34;opti3.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Final optimization problem for the trajectories case. Source: The paper
  &lt;/figcaption&gt;


&lt;/figure&gt;


&lt;p&gt;&lt;em&gt;p&lt;/em&gt; is as defined in the previous section. But what is &lt;em&gt;k&lt;/em&gt; in the summation? It is the number of policies apart from the optimal policy we are considering at that step of the algorithm. This will be clear in a second. The algorithm is as follows:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Start with the optimal policy $\pi^*$ and another random policy $\pi_{1}$. Find the $\alpha$ values that satisfy the above with &lt;em&gt;k&lt;/em&gt;=1. Hence find &lt;em&gt;R&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;Now using the &lt;em&gt;R&lt;/em&gt; we just found, find $\pi_{2}$ that maximizes $V^{\pi_{2}}(s_{o})$. This can be done using some RL algorithm.&lt;/li&gt;
&lt;li&gt;Now add $\pi_{2}$ to our current set of policies and optimize the above for $\pi^*$, $\pi_{1}$ and $\pi_{2}$ with &lt;em&gt;k&lt;/em&gt;=2.&lt;/li&gt;
&lt;li&gt;Keep going until you are &amp;ldquo;satisfied&amp;rdquo;.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;I will not get into the experiments conducted but I would highly recommend that you read the paper since there are some interesting details and observations.&lt;/p&gt;

&lt;h2 id=&#34;future-work&#34;&gt;Future Work&lt;/h2&gt;

&lt;p&gt;The authors plan on finding not just simple reward functions as they have done in this paper i.e. ones with small values but want to do one better to find &amp;ldquo;easy to learn&amp;rdquo; reward functions. I guess this means that &amp;ldquo;simple&amp;rdquo; doesn&amp;rsquo;t always mean &amp;ldquo;easy to learn&amp;rdquo;. They also want a way to be able to account for variation and noise in the state space and action selection process in real world applications. They also hope to find &amp;ldquo;locally consistent&amp;rdquo; reward functions in specific regions of the state space if they find that observed behaviour is far from optimal.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Modeling RL Problems</title>
      <link>https://skandavaidyanath.github.io/post/modeling-rl-problems/</link>
      <pubDate>Thu, 16 Jan 2020 22:12:05 +0530</pubDate>
      <guid>https://skandavaidyanath.github.io/post/modeling-rl-problems/</guid>
      <description>

&lt;p&gt;I recommend that you gather some RL basics before you proceed to read this article. The first couple of posts from &lt;a href=&#34;https://skandavaidyanath.github.io/courses/rl-course/&#34; target=&#34;_blank&#34;&gt;the course&lt;/a&gt; on my page could be a good start.&lt;/p&gt;

&lt;p&gt;In this article, I&amp;rsquo;m going to talk about something that I haven&amp;rsquo;t seen anywhere before and nobody really talks about it but I&amp;rsquo;m going to take a shot at it. I&amp;rsquo;ve learned RL on my own and hence I&amp;rsquo;ve read several articles on the internet about RL but most of them are about the different algorithms starting from the most basic dynamic programming and going on till the most complex Soft Actor Critic. But having spent some time conducting RL research, I find that nobody really talks about, according to me, the most challenging and interesting problem in RL &amp;ndash; the actual &lt;em&gt;modeling of the problem&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;So what do we mean by &amp;ldquo;modeling the problem&amp;rdquo;? If someone told you to solve the problem of self-driving cars using RL, how would you start? Lets say you decide to go ahead with some MDP framework, what would your states be? What would your actions be? What reward signal would you use? Would you solve the entire &amp;ldquo;self-driving&amp;rdquo; problem at once or would you want to break it down into smaller components? This is what I mean by &amp;ldquo;modeling a problem&amp;rdquo; and in my experience, has turned out to be the most important and challenging part of the bigger problem. Its something that sounds so simple but these early decisions are so important and I strongly believe that solid modeling could lead to better performance than using complex algorithms on weakly structured problems.&lt;/p&gt;

&lt;p&gt;This is in fact one of the things that drew me to RL because I thought modeling problems was extremely interesting and challenging as well. I also couldn&amp;rsquo;t find anything quite similar to this in other domains. Feature selection in supervised learning comes close but there are specific techniques and tests you can conduct to come up with the best set of features given a large feature set. So this seemed like a unique problem specific to the RL world. But if it is such an exciting problem why hasn&amp;rsquo;t anyone written about it? My guess is that there is no simple answer to the question of &amp;ldquo;How to model an RL problem?&amp;rdquo;. The answer is that &amp;ldquo;It depends on the problem&amp;rdquo;. But I do believe there are some very simple guidelines you can follow or rather questions you can ask yourself when you&amp;rsquo;re modeling your problem. So here we go! Five questions that will help you model RL problems. There&amp;rsquo;s one additional question I guess that I&amp;rsquo;m implicitly answering; we will stick to the MDP framework and not talk about SMDPs or POMDPs for now (if you&amp;rsquo;ve never heard of these, good).&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;What are the actions I need?&lt;/li&gt;
&lt;li&gt;Are my actions instantaneous?&lt;/li&gt;
&lt;li&gt;What is my state space and &lt;em&gt;can I make it smaller&lt;/em&gt;?&lt;/li&gt;
&lt;li&gt;How complex is my problem and can I split it into smaller and easier problems?&lt;/li&gt;
&lt;li&gt;What is the simplest reward function I can use?&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;So let&amp;rsquo;s go through them one by one.&lt;/p&gt;

&lt;h2 id=&#34;what-are-the-actions-i-need&#34;&gt;What are the actions I need?&lt;/h2&gt;

&lt;p&gt;This is the first question you need to ask yourself when you&amp;rsquo;re handed a problem. What is the agent going to achieve and what actions will it need to achieve that task? This decision is often the easiest to make because this comes along with the problem description. So if the task is to navigate a car from one position of a square grid to another, the directions of movement would probably be the most natural choice for actions. Although this is much harder to see in a problem like Chess or self-driving cars, with a little bit of effort, we can think about all the different actions we have at our disposal. The important thing to keep track of however, is whether the action space is continuous or discrete and if discrete, how many actions you have. This information could be useful to decide if you maybe want to split the problem up into smaller easier problems.
&lt;em&gt;Remember, the more actions you have, the harder the task is.&lt;/em&gt;&lt;/p&gt;














&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;taxi.png&#34; data-caption=&#34;The Taxi Problem. Source: Google Images&#34;&gt;
&lt;img src=&#34;taxi.png&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    The Taxi Problem. Source: Google Images
  &lt;/figcaption&gt;


&lt;/figure&gt;


&lt;h2 id=&#34;are-my-actions-instantaneous&#34;&gt;Are my actions instantaneous?&lt;/h2&gt;

&lt;p&gt;Once you&amp;rsquo;ve figured out what your actions are, we can take a closer look at them now. One thing we want to look for is whether the actions are instantaenous or not. Sometimes there could be actions that realistically take time to execute. For example, in the above car-grid problem, if your actions were &amp;ldquo;move the car to the green square&amp;rdquo;, this action could take time to execute realistically. Modeling them as instantaneous actions wouldn&amp;rsquo;t be accurate. In the MDP framework, we need all actions to be instanteneous. So what do we do when we have actions that take time? First, try to get rid of them or replace them with simpler instantaneous actions. But this is a trade-off because this could increase the number of actions. Next, think about whether the &amp;ldquo;long&amp;rdquo; action could be a separate problem on its own and that could be modeled as a separate smaller RL problem. But this starts getting into hierarchical RL and I wouldn&amp;rsquo;t get into it unless you&amp;rsquo;re sure about what I&amp;rsquo;m doing.
One thing that I&amp;rsquo;ve done in the past to model actions that take time (that could even vary, every time the action is played) is to model time using probabilities and put a variable in the state space (we&amp;rsquo;ll get to this) indicating that the crrent action is in progress. So where do the probabilities come into play? Lets say a &amp;ldquo;long&amp;rdquo; action is in progress. Now we keep playing an extra &amp;ldquo;WAIT&amp;rdquo; action or &amp;ldquo;NOP&amp;rdquo; action and flip a coin (not a fair one) and if its heads, the &amp;ldquo;long&amp;rdquo; action ends. This seems convoluted but I&amp;rsquo;ve found that sometimes it could make life a lot simpler by introducing not too much complexity into the state space or action space. Otherwise, simply foraying into SMDPs and hierarchical RL is fine as well if you&amp;rsquo;re confident.&lt;/p&gt;

&lt;h2 id=&#34;what-is-my-state-space-and-can-i-make-it-smaller&#34;&gt;What is my state space and can I make it smaller?&lt;/h2&gt;

&lt;p&gt;So what variables are important for my problem and what goes into my state space? The crucial step here is to think about your actions. What variables do you need to decide what action you want to play in each state? Even thinking about what information a human might need could be useful.
The key is to be minimalistic. Use as few variables as possible and use variables that don&amp;rsquo;t take too many values. Try to maintain a small, discrete state space. This is almost always never possible but its definitely worth the shot. If you can model a problem that you can solve with one of the basic RL algorithms without getting into function approximation (for the uninitiated, think of it as using deep learning), there&amp;rsquo;s nothing like it. But this is difficult and one of the hardest parts of the modeling problem.
It takes a long time at the start of your project and it could get frustrating because you&amp;rsquo;re working with a white board and a marker and not a keyboard and a monitor. But trust me, the effort will be worth it. If you can design a small, simple state space then half the battle is won. At USC, when I was modeling my problem, it took over two weeks but we reduced our state space from over 33 billion to just 1440!&lt;/p&gt;

&lt;h2 id=&#34;how-complex-is-my-problem-and-can-i-split-it-into-smaller-and-easier-problems&#34;&gt;How complex is my problem and can I split it into smaller and easier problems?&lt;/h2&gt;

&lt;p&gt;I&amp;rsquo;m afraid there is no easy way to answer this that applies to all RL problems. But let me give you an example that might help. If I&amp;rsquo;m trying to make a robot learn to play tennis, I might want to split the big problem into smaller problems of &amp;ldquo;learning how to serve&amp;rdquo; or &amp;ldquo;learning how to play a forehand&amp;rdquo;, etc. One way of figuring out whether your problem needs breaking down is to check if all the actions make sense with each other i.e. can you play all actions at almost all states? Do you need all the state variables to make a call on whether you want to play an action or can I judge some actions just based on a subset of state variables?
These questions may help you partition your state space and action space into multiple simpler problems that will make it easy to learn. Its not always easy to find these patterns and partitions though. And you also need to make sure there is an easy way to put these sub-problems back together. Nevertheless, if your problem is too complex i.e. too many states and/or too many actions then maybe it is worth spending the time solving multiple problems.&lt;/p&gt;

&lt;h2 id=&#34;what-is-the-simplest-reward-function-i-can-use&#34;&gt;What is the simplest reward function I can use?&lt;/h2&gt;

&lt;p&gt;And finally, my favourite question. Questions 3 and 5 are by far the hardest of the lot and this question is probably the hardest of the lot. Which makes writing the answer so much easier. Researchers have recognized the difficulty of crafting reward functions and created a new field called Inverse Reinforcement Learning just to address the issue. But once again, there are some guidelines you can follow.
Usually, I try to stick to the simplest reward function possible. What do I mean by simplest? Small values and as sparse as possible. So if I&amp;rsquo;m designing a reward function for chess, my rewards would only be at the end of the game and would be a +1, 0, -1 for a win, draw and loss respectively. Another reward function could be for every move played but this would be harder to craft and there&amp;rsquo;s no reason for you to try this until you know the simple reward function doesn&amp;rsquo;t work. But in the problem of chess, it is qite straight-forward but there are other problems where finding an elegant reward function might not be as simple.&lt;/p&gt;

&lt;p&gt;So there you have it. I hope that helped and hopefully when you model your next problem, these tips will help. But thinking about all of this modeling has made me think about inverse reinforcement learning and meta learning a lot more. Is it possible for RL models to learn the optimal design of a problem? Not just the reward function but also the state space and the action space? And if it can, can it learn some common properties that we can learn across all RL problems? These are tough questions that I hope to answer some day.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Works Like a Charm</title>
      <link>https://skandavaidyanath.github.io/post/works-like-a-charm/</link>
      <pubDate>Mon, 04 Nov 2019 03:10:24 +0530</pubDate>
      <guid>https://skandavaidyanath.github.io/post/works-like-a-charm/</guid>
      <description>&lt;p&gt;Coming soon&amp;hellip;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Bridging the Gaps With Reinforcement Learning</title>
      <link>https://skandavaidyanath.github.io/post/bridging-the-gaps-with-rl/</link>
      <pubDate>Mon, 04 Nov 2019 03:10:09 +0530</pubDate>
      <guid>https://skandavaidyanath.github.io/post/bridging-the-gaps-with-rl/</guid>
      <description>

&lt;p&gt;In this post, I will be talking about a unique way to use reinforcement learning (RL) in deep learning applications. I definitely recommend brushing up some deep learning fundamentals and if possible, some policy gradient fundamentals as well before you get started with this post.&lt;/p&gt;

&lt;p&gt;Traditionally, RL is used to solve sequential decision making problems in the video game space or robotics space or any other space where there is a concrete RL task at hand. There are several applications in the fields of video games and robotics where the task at hand can be very easily seen as an RL problem and can be modeled appropriately as well. But RL as a technique is quite versatile and can be used in several other domains to train neural networks that are traditionally trained in a supervised fashion. We&amp;rsquo;ll talk about one very important such application in this post. Along the way, I&amp;rsquo;ll also try to convince you that this isn&amp;rsquo;t really a different way to use RL but rather just a different way to look at the traditional RL problem. So with that, lets begin!&lt;/p&gt;

&lt;h2 id=&#34;non-differentiable-steps-in-deep-learning-the-gaps&#34;&gt;Non-differentiable steps in deep learning: The Gaps&lt;/h2&gt;

&lt;p&gt;Sometimes when we&amp;rsquo;re coming up with neural network architectures, we may need to incorporate some non-differentiable operations as a part of our network. Now this is a problem as we can&amp;rsquo;t backpropagate losses through such operation and hence lets call these &amp;ldquo;gaps&amp;rdquo;. So what are some common gaps we come across in neural networks?&lt;/p&gt;

&lt;p&gt;On a side-note, before we start talking about some &amp;ldquo;real gaps&amp;rdquo;, its worth mentioning that the famous ReLU function is a non-differentiable function but we overcome that gap by setting the derivative at 0 to either 1 or 0 and get away with it.&lt;/p&gt;

&lt;p&gt;Now lets take a better example &amp;ndash; variational autoencoders (VAE). Without going into two many details, the VAE network outputs two vectors: a $\mu$ vector and a $\sigma$ vector and it involves a crucial sampling step where we sample from the distribution &lt;em&gt;N($\mu$, $\sigma$)&lt;/em&gt; as a part of the network. Now sampling is a gap as it is a non-differentiable step. You should stop here and convince yourself that this is, in fact true. When we sample, we don&amp;rsquo;t know what the outcome will be and hence the function in not differentiable. So how do they get over this in the VAE case? They use a clever trick.
Instead of sampling from &lt;em&gt;N&lt;/em&gt;($\mu$, $\sigma$), they just rewrite this as $\mu$ + $\sigma$&lt;em&gt;N(0,1)&lt;/em&gt; where they sample from the standard normal function. This neat trick now makes the expression differentiable because we just need the $\mu$ and $\sigma$ quantities to be differentiable and we don&amp;rsquo;t care about the &lt;em&gt;N(0,1)&lt;/em&gt;. Remember that we only need to differentiate with respect to the parameters of our network (brush up some backpropagation basics if you&amp;rsquo;re confused here) and hence we need to differentiate only with respect to $\mu$ and $\sigma$ and not the standard normal distribution. For more details about VAEs read &lt;a href=&#34;https://towardsdatascience.com/intuitively-understanding-variational-autoencoders-1bfe67eb5daf&#34; target=&#34;_blank&#34;&gt;this post&lt;/a&gt; or &lt;a href=&#34;https://towardsdatascience.com/understanding-variational-autoencoders-vaes-f70510919f73&#34; target=&#34;_blank&#34;&gt;this one&lt;/a&gt;.&lt;/p&gt;














&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;vae.PNG&#34; data-caption=&#34;Variational Autoencoders. Source: here&#34;&gt;
&lt;img src=&#34;vae.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Variational Autoencoders. Source: &lt;a href=&#34;https://towardsdatascience.com/intuitively-understanding-variational-autoencoders-1bfe67eb5daf&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;
  &lt;/figcaption&gt;


&lt;/figure&gt;


&lt;p&gt;So as it turned out, that wasn&amp;rsquo;t a very good example either but we&amp;rsquo;re starting to understand what we mean by gaps now and how common they are. Some common examples of gaps in networks are sampling operations and the argmax operation. Once again, convince yourself that argmax is not a differentiable function. Assume you have a function that takes argmax of two quantities &lt;em&gt;(x1,x2)&lt;/em&gt;. When &lt;em&gt;x1&lt;/em&gt; &amp;gt; &lt;em&gt;x2&lt;/em&gt;, this has value 0 (zero-indexed) and when &lt;em&gt;x1&lt;/em&gt; &amp;lt; &lt;em&gt;x2&lt;/em&gt;, it has value 1. Say the function is not defined on the &lt;em&gt;x1==x2&lt;/em&gt; line or define it as you wish (0 or 1). Now if you can visualise the 2D plane, you&amp;rsquo;ll see that the function is not differentiable as we move across the &lt;em&gt;x1==x2&lt;/em&gt; line. So argmax isn&amp;rsquo;t differentiable &lt;em&gt;but max is a differentiable function&lt;/em&gt; (recall max pooling in CNNs). Read &lt;a href=&#34;https://www.reddit.com/r/MachineLearning/comments/4e2get/argmax_differentiable/&#34; target=&#34;_blank&#34;&gt;this thread&lt;/a&gt; for more details.&lt;/p&gt;

&lt;p&gt;These functions are commonly used in natural language processing (NLP) applications, information retrieval (IR) applications and Computer Vision (CV) applications as well. For example, a sampling function could be used to select words from a sentence based on a probability distribution in an NLP application or an argmax function could be used to find the highest ranked document in an IR application. &lt;a href=&#34;https://jhui.github.io/2017/03/15/Soft-and-hard-attention/&#34; target=&#34;_blank&#34;&gt;Hard attention&lt;/a&gt; uses sampling techniques which involves non-differentiable computation.&lt;/p&gt;

&lt;p&gt;So its quite clear that these gaps are common in several deep learning architectures and sometimes, it could even be useful to introduce such a gap in the network intentionally to reap added benefits. The only question is, &lt;em&gt;how do we bridge these gaps?&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&#34;reinforcement-learning-and-policy-gradients-the-bridge&#34;&gt;Reinforcement Learning and Policy Gradients: The Bridge&lt;/h2&gt;

&lt;p&gt;Policy gradients are a class of algorithms in RL. There are several policy gradient algorithms and &lt;a href=&#34;https://lilianweng.github.io/lil-log/2018/04/08/policy-gradient-algorithms.html&#34; target=&#34;_blank&#34;&gt;this&lt;/a&gt; is a great blog that lists out almost all of them. But without going into too many details, these algorithms work in the policy space by updating the parameters of the policy we&amp;rsquo;re trying to learn. That means we don&amp;rsquo;t necessarily need to find the value function of different states but we can directly alter our policy until we&amp;rsquo;re happy.
The most common policy gradient (PG) algorithm is the REINFORCE which is a Monte Carlo algorithm. This means we run an entire episode and make changes to our policy only at the end of each episode and not at every step. We make these changes based on the returns that we received by taking a given action from a given state in the episode. I skip the derivation of the policy gradient here but it can be found in the link above. The final result is in the image below.&lt;/p&gt;














&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;pg.PNG&#34; data-caption=&#34;The Policy Gradient. Source: here&#34;&gt;
&lt;img src=&#34;pg.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    The Policy Gradient. Source: &lt;a href=&#34;https://medium.com/@thechrisyoon/deriving-policy-gradients-and-implementing-reinforce-f887949bd63&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;
  &lt;/figcaption&gt;


&lt;/figure&gt;


&lt;p&gt;The key idea here is that in policy gradient methods, we are allowed to &lt;em&gt;sample different actions from a given state and wait till the end of an episode before we make updates to our network&lt;/em&gt;. So if we have a sampling operation as a part of our network, we can introduce a policy gradient and think of it as sampling actions in a given state in an RL framework. A similar procedure can also be followed if we had argmax in place of the sampling operation.&lt;/p&gt;

&lt;p&gt;Consider a neural network now with a gap. The images below are taken from &lt;a href=&#34;http://karpathy.github.io/2016/05/31/rl/&#34; target=&#34;_blank&#34;&gt;this blog&lt;/a&gt; on Policy Gradients written by Andrej Karpathy.&lt;/p&gt;

&lt;p&gt;












&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;karpathy1.PNG&#34; data-caption=&#34;Gaps in a neural network. Source: Karpathy&amp;rsquo;s blog&#34;&gt;
&lt;img src=&#34;karpathy1.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Gaps in a neural network. Source: Karpathy&amp;rsquo;s &lt;a href=&#34;http://karpathy.github.io/2016/05/31/rl/&#34; target=&#34;_blank&#34;&gt;blog&lt;/a&gt;
  &lt;/figcaption&gt;


&lt;/figure&gt;














&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;karpathy2.PNG&#34; data-caption=&#34;The sampling operation. Source: Karpathy&amp;rsquo;s blog&#34;&gt;
&lt;img src=&#34;karpathy2.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    The sampling operation. Source: Karpathy&amp;rsquo;s &lt;a href=&#34;http://karpathy.github.io/2016/05/31/rl/&#34; target=&#34;_blank&#34;&gt;blog&lt;/a&gt;
  &lt;/figcaption&gt;


&lt;/figure&gt;
&lt;/p&gt;

&lt;p&gt;So now we can train the blue arrows i.e. the differentiable path as usual. But to train the red arrows, we need to introduce a policy gradient and as we sample, we ensure with the help of the policy gradient that we encourage samples that led to a lower loss. In this way, we are &amp;ldquo;training&amp;rdquo; the sampling operation or one could say, propagating the loss through the sampling operation! Note that the updates to the red arrows happen independently than those of the blue arrows.
Note that in the diagrams above, there isn&amp;rsquo;t really a gap per-say because the blue arrows go all the way from start to finish. So there is a differentiable path and a non-differentiable path. A true gap would mean there would be no completely differentiable path at all. In this case, we need to make sure that the loss functions on either side of the gap are &amp;ldquo;in sync&amp;rdquo; and are being optimized in such a way that it facilitates joint training and achieves a common goal. This is often not as easy as it sounds.&lt;/p&gt;

&lt;p&gt;I said at the start that as obscure as it seems, this is still the traditional RL problem we&amp;rsquo;re used to with the MDP and states and actions. We can still look at this entire setup as a traditional RL problem if we think of the inputs to the neural network as the state and the sampling process as sampling different actions from that given state. Now what is the reward function? This depends on what comes after the gap and could be an output from the rest of the network or it could be a completely independent reward function that you came up with as well. So at the end of the day, it is still the same MDP with the traditional setup but just used in a very different way.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Deep RL for Conversational Recommender Systems</title>
      <link>https://skandavaidyanath.github.io/project/mpii/</link>
      <pubDate>Fri, 01 Nov 2019 04:46:38 +0530</pubDate>
      <guid>https://skandavaidyanath.github.io/project/mpii/</guid>
      <description>&lt;p&gt;This project is for my undergraduate thesis at the Max Planck Institute for Informatics under &lt;a href=&#34;https://andrewyates.net/&#34; target=&#34;_blank&#34;&gt;Dr. Andrew Yates&lt;/a&gt; and &lt;a href=&#34;https://paramitamirza.com/&#34; target=&#34;_blank&#34;&gt;Dr. Paramita Mirza&lt;/a&gt; and is currently in progress.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>The Human-Swarm Project</title>
      <link>https://skandavaidyanath.github.io/talk/usc-ict/</link>
      <pubDate>Sun, 21 Jul 2019 00:00:00 +0000</pubDate>
      <guid>https://skandavaidyanath.github.io/talk/usc-ict/</guid>
      <description></description>
    </item>
    
    <item>
      <title>The Human Swarm Project</title>
      <link>https://skandavaidyanath.github.io/project/usc-ict/</link>
      <pubDate>Sat, 20 Jul 2019 00:00:00 +0000</pubDate>
      <guid>https://skandavaidyanath.github.io/project/usc-ict/</guid>
      <description>&lt;p&gt;This project is currently in progress and a research paper is in preparation. I will attach the code, the slides and the paper once it has been published. This project was done under the guidance of &lt;a href=&#34;http://people.ict.usc.edu/~kgeorgila/&#34; target=&#34;_blank&#34;&gt;Prof. Kallirroi Georgila&lt;/a&gt; and &lt;a href=&#34;http://ict.usc.edu/profile/david-traum/&#34; target=&#34;_blank&#34;&gt;Prof. David Traum&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
